{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26269c34-f45e-4b6e-bd97-c3bfc015a4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,  8037, 23525,  ...,     0,     0,     0],\n",
      "        [    2,  3846,  6813,  ...,     0,     0,     0],\n",
      "        [    2,     1, 23531,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2, 20312, 28566,  ...,     0,     0,     0],\n",
      "        [    2, 18505, 28521,  ...,     0,     0,     0],\n",
      "        [    2,  7446,  8599,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertJapaneseTokenizer\n",
    "\n",
    "# データベース接続設定\n",
    "db_connection_string = \"postgresql://{}:{}@postgres-service:{}/{}\".format(\n",
    "    os.environ['POSTGRES_USER'], \n",
    "    os.environ['POSTGRES_PASSWORD'], \n",
    "    os.environ['POSTGRES_SERVICE_SERVICE_PORT'], \n",
    "    os.environ['POSTGRES_DB'])\n",
    "\n",
    "# データベースからデータを読み込む\n",
    "with psycopg2.connect(db_connection_string) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute('SELECT title FROM learning_resources')\n",
    "        records = cur.fetchall()\n",
    "        # レコードをテキストリストとして取得\n",
    "        texts = [record[0] for record in records]\n",
    "        \n",
    "# 日本語のBERTモデルとトークナイザーの読み込み\n",
    "model_name = \"cl-tohoku/bert-base-japanese\"\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# トークナイズと前処理の実行（最大長を指定）\n",
    "encoded_texts = tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,  # 最大トークン長を指定\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(encoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18370ba-ca05-435c-bd45-e947d315aae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa075f4-fa66-409b-a90b-97927df75122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d098df4-ac32-4166-85fa-6952845c4243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 事前学習済みモデルとトークナイザの読み込み\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# ドメイン特化データセットの読み込み（例：医療関連のテキスト）\n",
    "# 'path_to_your_domain_data.txt' は、ファインチューニングに使用するテキストファイルへのパスです\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"path_to_your_domain_data.txt\"})\n",
    "\n",
    "# データセットの前処理\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "\n",
    "# トレーニングの設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# トレーナーの初期化\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# ファインチューニングの実行\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ffb19-f688-4bad-b74d-9702e8999fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
